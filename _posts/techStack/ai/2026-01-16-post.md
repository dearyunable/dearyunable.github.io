---
title: "이미지 기반 어종 식별 및 정보 제공 서비스 만들기 (세번째 기록)"
excerpt: "Faster R-CNN 기본 개념 학습하기"
categories: [techStack, ai]
tags: [AI, CNN, ResNet50, FPN, RPN, RoI Head, Faster R-CNN]
last_modified_at: 2026-01-16T17:27:00+09:00
toc: true
toc_sticky: true
author_profile: true
sidebar:
    nav: "docs"
search: true
---

## [ Faster R-CNN ]
Faster R-CNN이란    
ResNet50을 backbone으로 사용하고,  
FPN과 Faster R-CNN 구조(RPN + RoI Head)를 결합한 객체 탐지 모델이다.  
(torchvision 구현에서는 FasterRCNNPredictor가 분류/회귀 head 역할을 한다.)  


## ResNet50
- 눈 역할 (특징 추출기, backbone)  
- 이미지의 특징 지도(Feature map)로 바꿔주는 눈.  
비유하자면, 사람 얼굴 사진을 보고 ResNet50은  
여기는 눈, 여기는 지느러미 같은 선, 여기 물고기 몸통 같은 덩어리 이런식으로  
특징 지도를 만든다.  
왜 ResNet50 일까?  
너무 얕으면 특징을 잡지 못하고 너무 깊으면 느리다. ResNet50이 정확도와 속도의 균형이 좋다.  

여기서 중요한 점은  
ResNet50은 "이게 물고기다" 라고 판단하지 않고, 형태, 선, 질감같은 특징만 뽑는다.  
즉, [CNN 기본 개념 학습]에서 공부했던 내용에서  
입력 이미지 → 합성곱(필터적용) → Feature Map → Pooling → 더 깊은 합성곱 → 더 추상적인 Feature Map  
이렇게 Conv + Pooling을 반복해서  
입력 이미지를 “특징(숫자)”으로 바꾸는 전체 과정을  
Feature Extractor라고 불리며, ResNet50이 이에 해당된다. 

Feature Extractor는 입력 이미지를 여러 합성곱 층을 거쳐  
의미 있는 Feature Map으로 변환하는 CNN 부분을 의미하며,  
구현에 따라 마지막 Feature Map만 사용하기도 하고,  
객체 탐지 모델에서는 중간 단계 Feature Map들을 함께 활용하기도 한다.  

ResNet50을 지나면 내부적으로 아래와 같은 Feature Map이 생긴다.  
C2: 256 × 56 × 56   ← 얕은 층 (세밀)  
C3: 512 × 28 × 28    
C4: 1024 × 14 × 14  
C5: 2048 × 7 × 7   ← 깊은 층 (추상적)  
= (채널 수 X 높이 X 너비)의 Feature Map(숫자들의 3차원 배열 - 높이, 너비는 딱 한 곳을 지정한게 아니라 격자 범위이다.)  
7x7=49 -> 49개는 위치(location, cell)의 개수이고,  
채널 수란 그 위치를 설명하는 '특징의 종류 개수' 이다.  

*** 예시는 이해를 돕기 위한 가상의 숫자이다.  
ex) 2048 × 7 × 7 => Feature Map에는 49개의 위치가 있고,각 위치마다 2048차원의 feature 벡터가 있다.***

이렇게 얕은 층~깊은 층 Feature Map이 전부 생성되는데  
일반 CNN은 마지막 깊은 Feature Map(채널 수 x 7 x 7)만 쓰고,  
객체 탐지 CNN에서는 각 층의 Feature Map(C2~C5)을 FPN을 거쳐 생성된 P2~P5 형태로 사용한다.  

여기서 알아야할 점이 있다.  
CNN에서 더 깊은 층이 될 수록 공간 크기를 줄이고 여러 픽셀을 하나의 값으로 요약한다.  
즉, 처음에는 픽셀 1칸이 작은 영역을 의미하고, 나중에 픽셀 1칸은 이미지의 넓은 영역을 의미하게 된다.  

|층 깊이|Feature Map|성격 위치정보|
|:---:|:---:|:---:|
|얕은 층|선, 모서리|매우 정확|
|중간 층|형태, 윤곽|중간|
|깊은 층|물고기|매우 러프|

-> 깊은 층일수록 의미는 강해지고, 위치는 희생된다.  
즉, 깊은 층의 한 위치는 원본 이미지의 넓은 영역을 대표하게 되어 정확한 좌표를 찍기에는 부적합해진다.
(사람이 보면 해상도가 깨진 것처럼 보이는 표현으로 변한다. 즉, 추상적으로 변한다.)  
하지만 객체 탐지는 "위치"가 필요하다.  

그래서 FPN이 등장한다.  

## FPN (Feature Pyramid Network)  
FPN은 '무엇인지'는 깊은 층에서 가져오고, '어디인지'는 얕은 층의 격자를 그대로 써서, 둘을 더해 하나의 Feature Map으로 만드는 것이다.
먼저, "얕은 층"과 "깊은 층"이 실제로 뭘까?  
CNN을 통과하면 이런 Feature Map이 생긴다고 했다.  

얕은 층 (C2): 56×56  ← 격자가 촘촘함 (한 칸이 작은 영역을 의미, 위치 정확)  
중간 층 (C3): 28×28  
깊은 층 (C4): 14×14  
아주 깊은 층 (C5): 7×7   ← 격자가 큼 (한 칸이 아주 큰 영역을 의미, 위치 거침)  

얕은 층의 "정확한 위치정보"와  
깊은 층의 "의미"를 합치자! 라는 생각이 FPN의 핵심 아이디어이다.  
그 방법인 의미는 위에서 아래로(깊은 층에서 얕은 층으로) 내려보내고  
위치는 아래에서 그대로 유지하겠다는 것이다.  

1. [의미를 위에서 아래로 내려보낸다]  
깊은 층(C5)의 Feature Map은  
7x7x(많은 채널)  
-> 이 안에는 "물고기일 확률 높음", "배경일 확률 낮음" 같은 의미 정보가 들어있다.  
FPN은 C5를 크게 늘린다.(업샘플링)  
7x7 → 14x14 → 28x28 → 56x56  
이미지를 키우는게 아니라 숫자 격자를 늘리는 것이다.  
이렇게 하면 의미는 그대로 위치 격자는 얕은 층과 맞아진다.  
FPN에서는 서로 다른 층의 Feature Map을 더하기 위해,  
각 층의 채널 수를 1×1 Convolution으로 동일하게 맞춘다.  
즉,   
얕은 층 (C2): 256x56×56  
중간 층 (C3): 595x28×28    
깊은 층 (C4): 788x14×14   
아주 깊은 층 (C5): 1025x7×7  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
얕은 층 (P2): 256x56×56  
중간 층 (P3): 256x28×28    
깊은 층 (P4): 256x14×14    
아주 깊은 층 (P5): 256x7×7  

=> 공간 크기는 그대로,    
채널 수만 전부 '256'으로 통일된다. (단, ‘C를 수정해서 쓰는 게 아니라, C를 재료로 P를 새로 만든다)  

2. [위치는 아래에서 유지]  
얕은 층(C2)의 Feature Map은 원래부터  
56x56  
각 칸이 이미지의 아주 작은 영역을 담당하며, 위치 정보가 매우 정확하다.  
FPN은 이걸 버리지 않는다.  

3. FPN은 같은 크기가 된 두 Feature Map을 아래와 같이 처리한다.  
업샘플링된 깊은 층 (의미 강함)  
&nbsp;&nbsp;&nbsp;&nbsp;\+    
얕은 층 Feature Map (의미 정확)  
&nbsp;&nbsp;&nbsp;&nbsp;\=  
새 Feature Map (P2)  

-> 이 연산은 이미지 합성이 아닌 같은 채널 위치끼리 element-wise 덧셈을 수행한다.  
이 결과 **정확한 좌표 위에 '물고기라는 의미'가 얹힌 지도가 추출된다.
공간 크기(H×W)는 각 층에서 동일하지만,  
채널 수는 256으로 통일되고,  
의미 정보가 보강된 ‘새로운 feature map(P2~P5)’가 만들어진다.  



## Faster R-CNN
Faster R-CNN은 이렇게 흘러간다.  
이미지  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
Backbone(CNN, 예: ResNet50)  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
(FPN이면 P2~P5 feature maps)  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
[1단계] RPN: “박스 후보” 뽑기  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
[2단계] RoI Head: 후보 박스만 “정밀 분류 + 박스 보정”  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
최종 박스들  

### [1단계] RPN (Region Proposal Network)  
RPN은 "물체가 있을 법한 위치만 골라내는 단계"이다.  
[RPN이 실제로 하는 일]  
	1) FPN feature map 위에 격자를 깐다.  
		- 입력: P2~P5  
	2) 각 위치마다 Anchor 여러 개를 둔다.  
		- 크기/비율이 다른 기본 박스들  
		- ex)작은/중간/큰x가로/세로/정사각형 -> 결과적으로 한 이미지 기준으로 수만 개의 anchor가 생성될 수 있다.  
		- 한 위치당 anchor 개수 = 보통 9개(3가지 크기 × 3가지 비율)  
		- P3가 50 × 80이면  
		- 위치 수 = 50 × 80 = 4,000  
		- anchor 수 = 4,000 × 9 = 36,000  
	3) 각 anchor마다 두 가지를 예측  
		- Objectness  
			: 이 박스 안에 "뭔가 있다/없다"  
			: 종류는 전혀 관심 없음  
		- Box offset  
			: anchor를 기준으로 얼마나 이동, 얼마나 키우거나 줄일지  
	4) 후보 정리**(여기가 핵심!)**  
		- objectness 점수 낮은 anchor 제거  
		- box offset 적용 → 실제 박스 좌표 생성  
		- NMS 적용 → 겹치는 박스 제거  
	5) 결과  
		- 출력1: proposal boxes(좌표)  
		- 출력2: objectness score(물체일 확률)  
		- 출력 형태: [ (x1, y1, x2, y2), 물체일 확률 ]  
					[ (x1, y1, x2, y2), 물체일 확률 ]  
					[ (x1, y1, x2, y2), 물체일 확률 ]  
					...  
		* proposal은 어떻게 만들어지나?  
			- anchor에 box offset 적용, objectness 점수 계산, NMS로 겹침제거를 통해 살아남은 박스들  
			
[전체 흐름 연결]  
Feature Map 한 위치  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
여러 Anchor 생성  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
각 Anchor에 대해  
  - Objectness (있다/없다)  
  - Box offset (조금 고치기)  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
점수 낮은 것 제거  
NMS로 겹침 제거  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
Proposal (물체 후보 박스)  


### [2단계] ROI Head - "후보만 정밀 검사"
RPN 이후의 형태: proposal = (x1, y1, x2, y2) + objectness score  
RPN이 만들어낸 proposal은  
“여기쯤에 뭔가 있을 것 같다”라는 후보 박스일 뿐이다.  
이제 이 후보들만을 대상으로 진짜 인식 작업을 수행하는 단계가  
바로 RoI Head이다.  

0. 입력: RPN이 고른 proposal + FPN의 Feature Map
	RoI Head의 입력은 두 가지다.  
	1) Proposal 박스들  
		- RPN이 골라낸 수백 개의 박스  
		- 위치와 크기만 있음  
		- 아직 클래스 정보 없음  
	2) Backbone/FPN의 Feature Map  
		- P2 ~ P5 (공간 구조 유지된 상태)  
	즉, “이 박스 위치에 해당하는 feature를 뽑아서 자세히 보자” 라는 단계다.  
	
1. **ROI Align - 박스마다 feature를 '같은 크기'로 추출**
- objectness score는 RoIAlign에는 사용 안 함 (정렬/선별용이지, feature 추출엔 안 씀)  
- 입력: proposal 좌표 (ex. x1, y1, x2, y2 =>> 10,20,120,120)  
	- 가로 = 110, 세로 = 100 => 면적 있음(유효한 proposal) => “이미지에서 이 영역을 봐라”라는 주소값  
- 출력: 256 × 7 × 7 → 그 위치의 feature 요약본  
- RoIAlign은 좌표를 ‘변형’하는 게 아니라 좌표를 ‘열쇠(key)’로 써서 FPN feature map에서 feature를 꺼내오는 단계  

문제는 proposal 박스의 크기가 제각각이라는 점이다.  
ROI Align은 proposal 박스가 가리키는 영역을 Feature Map에서 정확히 잘라내고 항상 같은 크기로 변환한다.  
proposal이 200개라면 200 x (256 x 7 x 7) 형태의 feature 묶음이 생성된다.  
즉, proposal 한 개당 하나의 256 x 7 x 7 출력값이 나온다.  
ROI Pooling과 달리 양자화(반올림)를 하지 않아  
위치 오차가 발생하지 않는다.  

2. **box_head (MLP Feature 변환 단계)**
RoI Align 결과: proposal x C x 7 x 7 (= 각 proposal당 256 × 7 × 7)  

이 상태는 공간 구조(C×H×W)를 가지므로,  
판단에 적합한 1차원 특징 벡터로 변환하기 위해 flatten을 수행한다.  

**Flatten (공간 펼치기)**  
256 × 7 × 7  
→ 256 × 49  
→ 12544  
- 격자(공간) 개념 제거 (이제부터는 “어디에 있었는지”보다 “어떤 특징들이 있었는지”가 중요)  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
**Fully Connected (MLP 통과, 보통 2-layer)**  
12544  
→ FC  
→ 1024  
→ FC  
→ 1024  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
고정 길이 벡터(예:1024)  
**결과 : proposal × 1024**  
여기서 하는 일은 feature를 “판단하기 좋은 형태”로 변환하는 것이다.  

3. **box_predictor (진짜 예측 단계)**
RoI Head는 RoI Align으로 얻은 feature를 두 개의 Linear layer에 넣어  
**클래스 점수**와 **박스 보정 값**을 예측한다.  
	1) Classfication Head ("이게 뭔가?")  
		- 입력: N × 1024  
		- 출력: N × num_classes  
		- background 포함하여 확률 계산  
		- 연어, 장어, 고등어 같은 구체적인 클래스 분류  
		
		=> 여기서 처음으로 '종류'를 판단한다.   
		
	2) Bounding Box Regression Head ("박스를 더 정확히")  
		- 입력: N × 1024  
		- 출력: N × (num_classes × 4) 또는 N × 4  
		- RPN이 만든 박스는 아직 거칠다 -> proposal 박스를 미세 조정  
		- 클래스별로 다른 보정값을 예측하기도 한다.  
		=> 이 단계가 끝나면 박스는 정답에 매우 가까워진다.   
		
4. 마지막 NMS - 최종 결과 정리
NMS란 겹치는 박스들 중에서 가장 좋은 것 하나만 남기고 나머지를 제거하는 규칙이다.  

지금까지 Faster R-CNN의 전체 흐름을 정리하면 다음과 같다.  

이미지  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
Backbone + FPN        ← feature map 생성  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
RPN                  ← "어디에 물체가 있을까?" (proposal)  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
▼▼ RoI Head ▼▼  
RoI Align             ← proposal마다 같은 크기 feature 추출  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
box_head (MLP)        ← feature를 벡터로 변환  
&nbsp;&nbsp;&nbsp;&nbsp;↓   
box_predictor         ← 실제 예측 (분류 + 박스보정)  

1. ResNet50은 입력 이미지를 여러 단계의 Feature Map(C2~C5)으로 변환한다.  
   이 단계에서는 물체의 종류를 판단하지 않고, 형태·선·질감 같은 특징만 추출한다.  

2. FPN은 얕은 층의 정확한 위치 정보와 깊은 층의 강한 의미 정보를 결합하여  
   P2~P5 형태의 균형 잡힌 Feature Map을 만든다.  

3. RPN은 이 Feature Map 위에서 수만 개의 anchor를 평가하여  
   물체가 있을 법한 후보 박스(proposal)만 추려낸다.  

4. RoI Head는 proposal마다 feature를 정밀하게 추출(RoI Align)한 뒤,  
   Linear layer를 통해  
   (1) 어떤 물체인지 분류하고  
   (2) 박스를 더 정확하게 보정한다.  

즉, Faster R-CNN은  
‘어디에 물체가 있을지’를 빠르게 거른 뒤,  
‘그게 무엇인지’를 정확하게 판단하는  
역할 분담 구조를 가진 객체 탐지 모델이다.  

이번 글에서는 Faster R-CNN의 구조와 개념을 중심으로 살펴보았다.    
앞으로는 분류 모델과 객체 탐지 모델을 실제 서비스에 적용하면서,  
코드 기준으로 하나씩 뜯어볼 계획이다.  

***
1. ResNet50 (Backbone)  
**- 무엇을 어떻게 하는지**  
: 원본 이미지를 CNN으로 통과시키며  
엣지 → 패턴 → 형태 → 의미로 점점 추상적인 특징을 추출함  

**- 수행 이후**  
: 한 이미지당 여러 단계의 feature map 생성  

C2:  256 × H/4  × W/4  
C3:  512 × H/8  × W/8  
C4: 1024 × H/16 × W/16  
C5: 2048 × H/32 × W/32  

2. FPN (Feature Pyramid Network)  
**- 무엇을 어떻게 하는지**  
: ResNet의 여러 층(C2~C5)을 결합하여  
위치 정보 + 의미 정보가 모두 잘 담긴 feature map을 만듦  

**- 수행 이후**
: 한 이미지당 여러 스케일의 FPN feature map 생성  

P2: 256 × H/4  × W/4  
P3: 256 × H/8  × W/8  
P4: 256 × H/16 × W/16  
P5: 256 × H/32 × W/32  

3. RPN (Region Proposal Network)  
**- 무엇을 어떻게 하는지**  
: FPN feature map 위의 각 위치마다 여러 anchor를 두고  
**“여기에 물체가 있을지”와 “박스를 어떻게 보정할지”**를 예측함  

**- 수행 이후**  
: 후보 정리를 거쳐 proposal 생성  
proposal = (x1, y1, x2, y2) + objectness score  

4. RoI Align
**- 무엇을 어떻게 하는지**  
: proposal 좌표를 이용해  
FPN feature map에서 해당 영역의 feature를 정확히 잘라냄  
크기가 달라도 항상 같은 크기로 변환  

**- 수행 이후**  
: proposal 하나당 고정 크기 feature 생성  
proposal마다 256 × 7 × 7  

5. box_head  
**- 무엇을 어떻게 하는지**  
: 256 × 7 × 7 feature를 펼친 뒤  
MLP를 통과시켜 판단하기 좋은 벡터로 변환  

**- 수행 이후**  
: proposal마다 1024  

6. box_predictor (최종 예측)  
**- 무엇을 어떻게 하는지**  
: 1024 feature를 입력으로 받아  
클래스 분류와 박스 미세 보정을 동시에 수행  

**- 수행 이후**  
: class score (num_classes)  
: bounding box (보정된 좌표)  