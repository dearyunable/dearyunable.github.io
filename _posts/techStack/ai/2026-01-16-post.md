---
title: "이미지 기반 어종 식별 및 정보 제공 서비스 만들기 (세번째 기록)"
excerpt: "Faster R-CNN 기본 개념 학습하기"
categories: [techStack, ai]
tags: [AI, CNN, ResNet50, FPN, RPN, RoI Head, Faster R-CNN]
last_modified_at: 2026-01-16T17:27:00+09:00
toc: true
toc_sticky: true
author_profile: true
sidebar:
    nav: "docs"
search: true
---

## [ Faster R-CNN ]
Faster R-CNN이란    
ResNet50을 backbone으로 사용하고,  
FPN과 Faster R-CNN 구조(RPN + RoI Head)를 결합한 객체 탐지 모델이다.  
(torchvision 구현에서는 FasterRCNNPredictor가 분류/회귀 head 역할을 한다.)  


## ResNet50
- 눈 역할 (특징 추출기, backbone)  
- 이미지의 특징 지도(Feature map)로 바꿔주는 눈.  
비유하자면, 사람 얼굴 사진을 보고 ResNet50은  
여기는 눈, 여기는 지느러미 같은 선, 여기 물고기 몸통 같은 덩어리 이런식으로  
특징 지도를 만든다.  
왜 ResNet50 일까?  
너무 얕으면 특징을 잡지 못하고 너무 깊으면 느리다. ResNet50이 정확도와 속도의 균형이 좋다.  

여기서 중요한 점은  
ResNet50은 "이게 물고기다" 라고 판단하지 않고, 형태, 선, 질감같은 특징만 뽑는다.  
즉, [CNN 기본 개념 학습]에서 공부했던 내용에서  
입력 이미지 → 합성곱(필터적용) → Feature Map → Pooling → 더 깊은 합성곱 → 더 추상적인 Feature Map  
이렇게 Conv + Pooling을 반복해서  
입력 이미지를 “특징(숫자)”으로 바꾸는 전체 과정을  
Feature Extractor라고 불리며, ResNet50이 이에 해당된다. 

Feature Extractor는 입력 이미지를 여러 합성곱 층을 거쳐  
의미 있는 Feature Map으로 변환하는 CNN 부분을 의미하며,  
구현에 따라 마지막 Feature Map만 사용하기도 하고,  
객체 탐지 모델에서는 중간 단계 Feature Map들을 함께 활용하기도 한다.  

ResNet50을 지나면 내부적으로 아래와 같은 Feature Map이 생긴다.  
C2: 256 × 56 × 56   ← 얕은 층 (세밀)  
C3: 512 × 28 × 28    
C4: 1024 × 14 × 14  
C5: 2048 × 7 × 7   ← 깊은 층 (추상적)  
= (채널 수 X 높이 X 너비)의 Feature Map(숫자들의 3차원 배열 - 높이, 너비는 딱 한 곳을 지정한게 아니라 격자 범위이다.)  
7x7=49 -> 49개는 위치(location, cell)의 개수이고,  
채널 수란 그 위치를 설명하는 '특징의 종류 개수' 이다.  

*** 예시는 이해를 돕기 위한 가상의 숫자이다.  
ex) 2048 × 7 × 7 => Feature Map에는 49개의 위치가 있고,각 위치마다 2048차원의 feature 벡터가 있다.***

이렇게 얕은 층~깊은 층 Feature Map이 전부 생성되는데  
일반 CNN은 마지막 깊은 Feature Map(채널 수 x 7 x 7)만 쓰고,  
객체 탐지 CNN에서는 각 층의 Feature Map(C2~C5)을 FPN을 거쳐 생성된 P2~P5 형태로 사용한다.  

여기서 알아야할 점이 있다.  
CNN에서 더 깊은 층이 될 수록 공간 크기를 줄이고 여러 픽셀을 하나의 값으로 요약한다.  
즉, 처음에는 픽셀 1칸이 작은 영역을 의미하고, 나중에 픽셀 1칸은 이미지의 넓은 영역을 의미하게 된다.  

|층 깊이|Feature Map|성격 위치정보|
|:---:|:---:|:---:|
|얕은 층|선, 모서리|매우 정확|
|중간 층|형태, 윤곽|중간|
|깊은 층|물고기|매우 러프|

-> 깊은 층일수록 의미는 강해지고, 위치는 희생된다.  
즉, 깊은 층의 한 위치는 원본 이미지의 넓은 영역을 대표하게 되어 정확한 좌표를 찍기에는 부적합해진다.
(사람이 보면 해상도가 깨진 것처럼 보이는 표현으로 변한다. 즉, 추상적으로 변한다.)  
하지만 객체 탐지는 "위치"가 필요하다.  

그래서 FPN이 등장한다.  

## FPN (Feature Pyramid Network)  
FPN은 '무엇인지'는 깊은 층에서 가져오고, '어디인지'는 얕은 층의 격자를 그대로 써서, 둘을 더해 하나의 Feature Map으로 만드는 것이다.
먼저, "얕은 층"과 "깊은 층"이 실제로 뭘까?  
CNN을 통과하면 이런 Feature Map이 생긴다고 했다.  

얕은 층 (C2): 56×56  ← 격자가 촘촘함 (한 칸이 작은 영역을 의미, 위치 정확)  
중간 층 (C3): 28×28  
깊은 층 (C4): 14×14  
아주 깊은 층 (C5): 7×7   ← 격자가 큼 (한 칸이 아주 큰 영역을 의미, 위치 거침)  

얕은 층의 "정확한 위치정보"와  
깊은 층의 "의미"를 합치자! 라는 생각이 FPN의 핵심 아이디어이다.  
그 방법인 의미는 위에서 아래로(깊은 층에서 얕은 층으로) 내려보내고  
위치는 아래에서 그대로 유지하겠다는 것이다.  

1. [의미를 위에서 아래로 내려보낸다]  
깊은 층(C5)의 Feature Map은  
7x7x(많은 채널)  
-> 이 안에는 "물고기일 확률 높음", "배경일 확률 낮음" 같은 의미 정보가 들어있다.  
FPN은 C5를 크게 늘린다.(업샘플링)  
7x7 → 14x14 → 28x28 → 56x56  
이미지를 키우는게 아니라 숫자 격차를 늘리는 것이다.  
이렇게 하면 의미는 그대로 위치 격자는 얕은 층과 맞아진다.  
FPN에서는 서로 다른 층의 Feature Map을 더하기 위해,  
각 층의 채널 수를 1×1 Convolution으로 동일하게 맞춘다.  
실제 구현에서는 보통 256 채널로 통일한다.  

2. [위치는 아래에서 유지]  
얕은 층(C2)의 Feature Map은 원래부터  
56x56  
각 칸이 이미지의 아주 작은 영역을 담당하며, 위치 정보가 매우 정확하다.  
FPN은 이걸 버리지 않는다.  

3. FPN은 같은 크기가 된 두 Feature Map을 아래와 같이 처리한다.  
업샘플링된 깊은 층 (의미 강함)  
&nbsp;&nbsp;&nbsp;&nbsp;\+    
얕은 층 Feature Map (의미 정확)  
&nbsp;&nbsp;&nbsp;&nbsp;\=  
새 Feature Map (P2)  

-> 이 연산은 이미지 합성이 아닌 같은 채널 위치끼리 element-wise 덧셈을 수행한다.  
이 결과 **정확한 좌표 위에 '물고기라는 의미'가 얹힌 지도가 추출된다.  


## Faster R-CNN
Faster R-CNN은 이렇게 흘러간다.  
이미지  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
Backbone(CNN, 예: ResNet50)  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
(FPN이면 P2~P5 feature maps)  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
[1단계] RPN: “박스 후보” 뽑기  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
[2단계] RoI Head: 후보 박스만 “정밀 분류 + 박스 보정”  
&nbsp;&nbsp;&nbsp;&nbsp;↓  
최종 박스들  

### [1단계] RPN (Region Proposal Network)  
RPN은 "물체가 있을 법한 위치만 골라내는 단계"이다.  
[RPN이 실제로 하는 일]  
	1) Feature Map 위에 격자를 깐다.  
		- Feature Map의 각 위치를 기준으로 판단함  
	2) 각 위치마다 Anchor 여러 개를 둔다. (Feature Map이 256x7x7 일 때, 전체 anchor수=7x7x(위치당 anchor 수))  
		- 크기/비율이 다른 기본 박스들  
		- ex)작은/중간/큰x가로/세로/정사각형 -> 결과적으로 한 이미지 기준으로 수만 개의 anchor가 생성될 수 있다.  
	3) 각 anchor마다 두 가지를 예측  
		- Objectness  
			: 이 박스 안에 "뭔가 있다/없다"  
			: 종류는 전혀 관심 없음  
		- Box offset  
			: anchor를 실제 물체에 맞게 조금 이동하거나 키우고 줄이기   
	4) 후보 정리**(여기가 핵심!)**  
		- objectness 점수 낮은 것 제거  
		- 겹치는 박스는 NMS로 정리(거의 같은 박스면 점수 높은 것만 남김)  
	5) 결과  
		- 수만 개 anchor → 수백 개 proposal  
		- 이 proposal은 "여기엔 물체가 있을 가능성이 높다" 하지만 아직 뭐인지는 모름  
		* proposal은 어떻게 만들어지나?  
			- anchor에 box offset 적용, objectness 점수 계산, NMS로 겹침제거를 통해 살아남은 박스들  
			
[전체 흐름 연결]  
Feature Map 한 위치  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
여러 Anchor 생성  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
각 Anchor에 대해  
  - Objectness (있다/없다)  
  - Box offset (조금 고치기)  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
점수 낮은 것 제거  
NMS로 겹침 제거  
 &nbsp; &nbsp; &nbsp; &nbsp;↓  
Proposal (물체 후보 박스)  


### [2단계] ROI Head - "후보만 정밀 검사"
RPN이 만들어낸 proposal은  
“여기쯤에 뭔가 있을 것 같다”라는 후보 박스일 뿐이다.  
이제 이 후보들만을 대상으로 진짜 인식 작업을 수행하는 단계가  
바로 RoI Head이다.  

1. 입력: RPN이 고른 proposal + Feature Map
	RoI Head의 입력은 두 가지다.  
	1) Proposal 박스들  
		- RPN이 골라낸 수백 개의 박스  
		- 위치와 크기만 있음  
		- 아직 클래스 정보 없음  
	2) Backbone/FPN의 Feature Map  
		- P2 ~ P5 (공간 구조 유지된 상태)  
	즉, “이 박스 위치에 해당하는 feature를 뽑아서 자세히 보자” 라는 단계다.  
	
2. **ROI Align - 박스마다 feature를 '같은 크기'로 추출**
문제는 proposal 박스의 크기가 제각각이라는 점이다.  
ROI Align은 proposal 박스가 가리키는 영역을 Feature Map에서 정확히 잘라내고 항상 같은 크기로 변환한다.  
proposal이 200개라면 200 x (C x 7 x 7) 형태의 feature 묶음이 생성된다.  
ROI Pooling과 달리 양자화(반올림)를 하지 않아  
위치 오차가 발생하지 않는다.  

3. RoI Head의 실제 예측 2가지
RoI Head는 RoI Align으로 얻은 feature를 두 개의 Linear layer에 넣어  
**클래스 점수**와 **박스 보정 값**을 예측한다.  
	1) Classfication ("이게 뭔가?")  
		- 연어, 장어, 고등어 같은 구체적인 클래스 분류  
		- background 포함하여 확률 계산  
		=> 여기서 처음으로 '종류'를 판단한다.  
		
	2) Bounding Box Regression ("박스를 더 정확히")  
		- RPN이 만든 박스는 아직 거칠다  
		- ROI Head에서 한 번 더 박스를 미세 조정한다.  
		- 클래스별로 다른 보정값을 예측하기도 한다.  
		=> 이 단계가 끝나면 박스는 정답에 매우 가까워진다.  
		
4. 마지막 NMS - 최종 결과 정리
NMS란 겹치는 박스들 중에서 가장 좋은 것 하나만 남기고 나머지를 제거하는 규칙이다.  

지금까지 Faster R-CNN의 전체 흐름을 정리하면 다음과 같다.  

1. ResNet50은 입력 이미지를 여러 단계의 Feature Map(C2~C5)으로 변환한다.  
   이 단계에서는 물체의 종류를 판단하지 않고, 형태·선·질감 같은 특징만 추출한다.  

2. FPN은 얕은 층의 정확한 위치 정보와 깊은 층의 강한 의미 정보를 결합하여  
   P2~P5 형태의 균형 잡힌 Feature Map을 만든다.  

3. RPN은 이 Feature Map 위에서 수만 개의 anchor를 평가하여  
   물체가 있을 법한 후보 박스(proposal)만 추려낸다.  

4. RoI Head는 proposal마다 feature를 정밀하게 추출(RoI Align)한 뒤,  
   Linear layer를 통해  
   (1) 어떤 물체인지 분류하고  
   (2) 박스를 더 정확하게 보정한다.  

즉, Faster R-CNN은  
‘어디에 물체가 있을지’를 빠르게 거른 뒤,  
‘그게 무엇인지’를 정확하게 판단하는  
역할 분담 구조를 가진 객체 탐지 모델이다.  

이번 글에서는 Faster R-CNN의 구조와 개념을 중심으로 살펴보았다.    
앞으로는 분류 모델과 객체 탐지 모델을 실제 서비스에 적용하면서,  
코드 기준으로 하나씩 뜯어볼 계획이다.  